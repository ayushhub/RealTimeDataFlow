<html>
<head>
<title>KafkaStreaming.scala</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.ln { color: #999999; font-weight: normal; font-style: normal; }
.s0 { color: rgb(0,0,0); }
</style>
</head>
<BODY BGCOLOR="#ffffff">
<TABLE CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<TR><TD><CENTER>
<FONT FACE="Arial, Helvetica" COLOR="#000000">
KafkaStreaming.scala</FONT>
</center></TD></TR></TABLE>
<pre>
<a name="l1"><span class="ln">1    </span></a><span class="s0">package com.vishnu.spark.streaming 
<a name="l2"><span class="ln">2    </span></a> 
<a name="l3"><span class="ln">3    </span></a>import org.apache.spark.streaming.kafka._ 
<a name="l4"><span class="ln">4    </span></a>import org.apache.spark._ 
<a name="l5"><span class="ln">5    </span></a>import org.apache.spark.streaming._ 
<a name="l6"><span class="ln">6    </span></a>import org.apache.spark.streaming.StreamingContext._ 
<a name="l7"><span class="ln">7    </span></a> 
<a name="l8"><span class="ln">8    </span></a>/** 
<a name="l9"><span class="ln">9    </span></a> * @author : vishnu viswanath 
<a name="l10"><span class="ln">10   </span></a> * Receiver based approach, i.e., used kafka consumer api to implement receiver 
<a name="l11"><span class="ln">11   </span></a> * Drawback : possible loss of data incase of failures 
<a name="l12"><span class="ln">12   </span></a> *  
<a name="l13"><span class="ln">13   </span></a> * Solution : use write-ahead logs and Reliable receivers. 
<a name="l14"><span class="ln">14   </span></a> * Spark provides a built in ReliableKafkaReceiver class which is not used by default. 
<a name="l15"><span class="ln">15   </span></a> * To use this receiver, set spark.streaming.receiver.writeAheadLog.enable to true  
<a name="l16"><span class="ln">16   </span></a> *   
<a name="l17"><span class="ln">17   </span></a> *   
<a name="l18"><span class="ln">18   </span></a> */ 
<a name="l19"><span class="ln">19   </span></a>object KafkaStreaming { 
<a name="l20"><span class="ln">20   </span></a>   
<a name="l21"><span class="ln">21   </span></a>  def main(args: Array[String]) { 
<a name="l22"><span class="ln">22   </span></a>    val conf = new SparkConf().setAppName(&quot;KafkaStreaming&quot;).setMaster(&quot;spark://Vishnus-MacBook-Pro.local:7077&quot;) 
<a name="l23"><span class="ln">23   </span></a>    val ssc = new StreamingContext(conf, Seconds(1)) 
<a name="l24"><span class="ln">24   </span></a>     
<a name="l25"><span class="ln">25   </span></a>    //default zookeeper quorum is localhost in single node setup 
<a name="l26"><span class="ln">26   </span></a>    val zqQuorum = &quot;localhost&quot; 
<a name="l27"><span class="ln">27   </span></a>    val groupId = &quot;spark&quot; 
<a name="l28"><span class="ln">28   </span></a>    val topics = &quot;spark_streaming&quot; 
<a name="l29"><span class="ln">29   </span></a>    val topicMap = topics.split(&quot;,&quot;).map((_, 1)).toMap 
<a name="l30"><span class="ln">30   </span></a>    val lines = KafkaUtils.createStream(ssc,zqQuorum,groupId,topicMap) 
<a name="l31"><span class="ln">31   </span></a>    val words = lines.map(_._2).flatMap(_.split(&quot; &quot;)) 
<a name="l32"><span class="ln">32   </span></a>    val pairs = words.map(word =&gt; (word,1)) 
<a name="l33"><span class="ln">33   </span></a>    val wordCounts = pairs.reduceByKey(_+_) 
<a name="l34"><span class="ln">34   </span></a>    wordCounts.print() 
<a name="l35"><span class="ln">35   </span></a>     
<a name="l36"><span class="ln">36   </span></a>    ssc.start() 
<a name="l37"><span class="ln">37   </span></a>    ssc.awaitTermination() 
<a name="l38"><span class="ln">38   </span></a>  } 
<a name="l39"><span class="ln">39   </span></a>}</span></pre>
</body>
</html>